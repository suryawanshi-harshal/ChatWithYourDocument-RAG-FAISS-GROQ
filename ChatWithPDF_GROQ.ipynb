{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your_GROQ_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\HarshalSuryawanshi\\AppData\\Local\\Temp\\ipykernel_8112\\105956271.py:17: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  filepath = \"C:\\\\Users\\HarshalSuryawanshi\\\\OneDrive - Ternpoint Solutions LP\\\\Documents\\\\ChatWithPDF\\\\TPS Data Engineering setup guide.docx\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_document(filepath):\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "\n",
    "    if file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(filepath)\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        loader = UnstructuredWordDocumentLoader(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: {}\".format(file_extension))\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "filepath = \"C:\\\\Users\\HarshalSuryawanshi\\\\OneDrive - Ternpoint Solutions LP\\\\Documents\\\\ChatWithPDF\\\\TPS Data Engineering setup guide.docx\"\n",
    "docs = load_document(filepath)\n",
    "\n",
    "# Set up the text splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "splitted_docs = r_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HarshalSuryawanshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\HarshalSuryawanshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# Create a custom embedding function\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name='all-mpnet-base-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "# Initialize the embedding function\n",
    "embeddings = SentenceTransformerEmbeddings()\n",
    "\n",
    "# Create the FAISS index\n",
    "vector_store = FAISS.from_documents(splitted_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key = \"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\",\n",
    "    model_name = 'mixtral-8x7b-32768'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following quesion based on the retrieved context.\n",
    "    Think step by step. Keep your answers brief. Reply with \"I'm not sure how to answer that. Please provide more context and try simplyfying your question.\" when you don't have an answer for any question. \n",
    "    Do not respond with wrong answers. All answers should come form the retireved context.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm_chain = create_stuff_documents_chain(\n",
    "    llm, \n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever, \n",
    "    llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a warehouse, follow these steps:\n",
      "\n",
      "2. You need the necessary permissions to create a warehouse in your fabric\n",
      "    workspace. Make sure you have the required role and access level to\n",
      "    perform this action.\n",
      "3. Once you are in the warehouse, click on \"Get Data\" -> \"New Dataflow Gen2\".\n",
      "4. On the Dataflow screen, click on \"Get data\" -> \"More\" -> Select the\n",
      "    lakehouse that you have your data residing in.\n",
      "5. On the \"Connect to data source\" popup, click on \"Connect\". If there isn't\n",
      "    any connection, you will need to create one with the required\n",
      "    permissions.\n",
      "6. Choose the tables that you want and click on \"Create\".\n",
      "7. Make sure the data destination is set to your warehouse for all the tables\n",
      "    you want to publish there.\n",
      "8. Click on \"publish\" and you should see your tables in the warehouse after\n",
      "    successfully executing the dataflow.\n",
      "\n",
      "To summarize, you need to have the required role and access level to create a\n",
      "warehouse and connect to the data source. Additionally, the credential used for\n",
      "connecting to the data source should have the \"Storage Blob Data Contributor\"\n",
      "role assigned on the storage account.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def format_response(response):\n",
    "    # Split the response into paragraphs\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', response)\n",
    "    \n",
    "    formatted_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        # Check if the paragraph is a numbered list\n",
    "        if re.match(r'\\d+\\.', paragraph):\n",
    "            # Split the numbered list into items\n",
    "            items = re.split(r'\\n\\s*(\\d+\\.)', paragraph)\n",
    "            formatted_items = []\n",
    "            for i in range(1, len(items), 2):\n",
    "                number = items[i]\n",
    "                text = items[i+1] if i+1 < len(items) else \"\"\n",
    "                wrapped_text = textwrap.fill(text.strip(), width=76, subsequent_indent='    ')\n",
    "                formatted_items.append(f\"{number} {wrapped_text}\")\n",
    "            formatted_paragraphs.append('\\n'.join(formatted_items))\n",
    "        else:\n",
    "            # Regular paragraph\n",
    "            wrapped = textwrap.fill(paragraph.strip(), width=80)\n",
    "            formatted_paragraphs.append(wrapped)\n",
    "    \n",
    "    # Join paragraphs with double line breaks\n",
    "    formatted_response = '\\n\\n'.join(formatted_paragraphs)\n",
    "    \n",
    "    return formatted_response\n",
    "\n",
    "# Use the function\n",
    "prompt = \"Detailed steps to create a warehouse. Explain any required permissions too.\"\n",
    "response = retrieval_chain.invoke({\"input\": prompt})\n",
    "formatted_answer = format_response(response['answer'])\n",
    "print(formatted_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command to start the environment - chatbot-env\\Scripts\\activate\n",
    "run the sript - python interactive_ChatWithPDF_GROQ.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
